{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'gridworld' object has no attribute 'n_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m world2\u001b[38;5;241m.\u001b[39mcalc_p_matrix()\n\u001b[0;32m      7\u001b[0m world2\u001b[38;5;241m.\u001b[39mcalc_r_matrix()\n\u001b[1;32m----> 9\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[43mworld0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_states\u001b[49m)\n\u001b[0;32m     10\u001b[0m action_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(actions))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'gridworld' object has no attribute 'n_states'"
     ]
    }
   ],
   "source": [
    "world0 = gridworld((5,5),(2,4),[(2,2),(2,0)],[(1,1),(2,1),(1,3),(2,3)],[(4,0),(4,1),(4,2),(4,3),(4,4)],p_e=0.2,dest_rewards=[1,10],haz_rewards=-1,dest_names=['diddy reese','safron rose'])\n",
    "world1 = gridworld((5,5),(2,4),[(2,2),],[(1,1),(2,1),(1,3),(2,3)],[(4,0),(4,1),(4,2),(4,3),(4,4)],p_e=0.2,dest_rewards=[1,],haz_rewards=-1,dest_names=['diddy reese',])\n",
    "world2 = gridworld((5,5),(2,4),[(2,0),],[(1,1),(2,1),(1,3),(2,3)],[(4,0),(4,1),(4,2),(4,3),(4,4)],p_e=0.2,dest_rewards=[10,],haz_rewards=-1,dest_names=['safron rose',])\n",
    "\n",
    "world1.calc_p_matrix()\n",
    "world1.calc_r_matrix()\n",
    "world2.calc_p_matrix()\n",
    "world2.calc_r_matrix()\n",
    "\n",
    "states = np.arange(world0.num_states)\n",
    "action_indices = np.arange(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_model(world, state, action, state_next):\n",
    "    if state < world.num_states and state_next < world.num_states and action < 5:\n",
    "        return world.p_matrix[action, state, state_next]\n",
    "    else:\n",
    "        print('Invalid state or action')\n",
    "        return None\n",
    "\n",
    "def reward_function(world, state, action, state_next):\n",
    "    if state < world.num_states and state_next < world.num_states and action < 5:\n",
    "        return world.r_matrix[action, state, state_next]\n",
    "    else:\n",
    "        print('Invalid state or action')\n",
    "        return None\n",
    "\n",
    "def value_iteration(world, gamma, epsilon):\n",
    "    states = np.arange(world.num_states)\n",
    "    # Initialize value function\n",
    "    value_map = {s: 0 for s in states}\n",
    "    action_indices = np.arange(len(actions))\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = value_map[s]\n",
    "            value_map[s] = max(sum(transition_model(world,s, a, s_next) * (reward_function(world, s, a, s_next) + gamma * value_map[s_next])\n",
    "                            for s_next in states) for a in action_indices) # value function\n",
    "            delta = max(delta, abs(v - value_map[s])) # amount in which value function has changed\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    return value_map\n",
    "\n",
    "def policy_extraction(world, value_map, gamma):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        # find the max a in actions that maximize the function given by lambda\n",
    "        policy[s] = max(action_indices, key=lambda a: sum(transition_model(world, s, a, s_next) * (reward_function(world, s, a, s_next) + gamma * value_map[s_next])\n",
    "                                                    for s_next in states))\n",
    "    return policy\n",
    "\n",
    "def get_policy_map(world, policy):\n",
    "    policy_map = np.zeros((world.x_max,world.y_max),dtype = str)\n",
    "    action_signs = ['o','→','←','↑','↓']\n",
    "    for i in range(world.y_max):\n",
    "        for j in range(world.x_max):\n",
    "            policy_map[world.y_max -1 -i][j] = action_signs[policy[i*world.x_max+j]]\n",
    "    print(policy_map)\n",
    "    return policy_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_map1 = value_iteration(world1, 0.8, 0.01)\n",
    "value_map2 = value_iteration(world2, 0.8, 0.01)\n",
    "policy1 = policy_extraction(world1, value_map1, 0.8)\n",
    "policy2 = policy_extraction(world2, value_map2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_map = {s: value_map1[s] + value_map2[s] for s in states}\n",
    "policy = policy_extraction(world1, value_map, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['↓' '←' '→' '↓' '←']\n",
      " ['↓' 'o' 'o' '↓' '←']\n",
      " ['→' '→' 'o' '←' '←']\n",
      " ['↑' 'o' 'o' '↑' '←']\n",
      " ['↑' '←' '→' '↑' '←']]\n",
      "\n",
      "[['↓' '←' '→' '↓' '←']\n",
      " ['↓' 'o' 'o' '↓' '←']\n",
      " ['↓' '←' '→' '↓' '↓']\n",
      " ['↓' 'o' 'o' '↓' '←']\n",
      " ['→' '→' 'o' '←' '←']]\n",
      "\n",
      "[['↓' '←' '→' '↓' '←']\n",
      " ['↓' 'o' 'o' '↓' '←']\n",
      " ['↓' '→' '→' '↓' '←']\n",
      " ['↓' 'o' 'o' '↓' '←']\n",
      " ['→' '→' 'o' '←' '←']]\n"
     ]
    }
   ],
   "source": [
    "policy_map1 = get_policy_map(world1, policy1)\n",
    "print()\n",
    "policy_map2 = get_policy_map(world2, policy2)\n",
    "print()\n",
    "policy_map = get_policy_map(world0, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
